\chapter{Implementação da Ferramenta \tjscraper~\label{chp:implementação-da-ferramenta}}

\section{Organização geral}

A ferramenta foi projetada como uma biblioteca de \textit{software} acompanhada
de duas interfaces de usuário~\footnote{Nesta seção, ``usuário'' refere-se a
alguém que não esteja utilizando a ferramenta como uma biblioteca de software,
e sim como aplicação.}: uma aplicação de Interface de Linha de Comando
(ILC\footnote{Comumente referenciado com a sigla em inglês ``CLI'', de
``Command Line Interface''.}) e uma aplicação de servidor Web.

A ILC tem como objetivo permitir o uso de boa parte dos diferentes recursos da
biblioteca em um terminal para a execução tarefas pontuais, como exibir
informações sobre estado atual da \textit{cache}, iniciar um processo de
extração, ou mesmo iniciar uma instância de servidor Web da ferramenta para
fins de desenvolvimento.

O servidor Web é destinado ao usuário que queira uma interface de simples
acesso e uso da ferramenta, primariamente focando na obtenção de dados dos TJs
com os recursos que não estejam disponíveis nas páginas oficiais dos TJs
(descritos na~\Cref{sec:Motivação-e-Contexto}), tendo em mente como público
alvo jornalistas.

\section{Ferramentas utilizadas~\label{section:ferramentas-utilizadas}}

Visando fomentar a iniciativa e movimento de Software-Livre, tomou-se como
exigência própria para a implementação que todas as ferramentas, assim como a
próprio \tjscraper, fossem Software-Livre por, além de outros fatores, questões
de reprodutibilidade e auditoria.

Para a implementação da \tjscraper, escolheu-se Python 3.10 como linguagem
principal considerando os seguintes critérios técnicos: a premissa de que o
gargalo do tempo de busca seria o de IO de Rede (para busca dos processos nos
servidores dos TJs) ou de Armazenamento (para buscas na \textit{cache}); não se
tratando de um problema inerentemente atrelado à CPU\footnote{Tradução livre
para o termo em inglês ``\textit{CPU-bound}''.}, não seriam prioritárias
otimizações agressivas de compilador tanto AOT nem
JIT\footnote{``Ahead-Of-Time'' --- antes da execução do programa --- e
``Just-In-Time'' --- durante a execução do programa ---, respectivamente.};
familiaridade do autor com a linguagem; disponibilidade de recursos de
abstração de alto-nível na biblioteca padrão da linguagem ou em bibliotecas
externas para as principais operações da ferramenta.

O gerenciamento de dependências e isolamento do ambiente de desenvolvimento foi
feito utilizando \cite{about:poetry} visando tanto a reprodutibilidade dos
resultados e funcionamento da ferramenta quanto facilitar seu empacotamento e
distribuição. A ILC foi implementada via \cite{about:typer} pelas garantias de
corretude de tipagem. Para a interface Web, foi escolhido \cite{about:flask}
por conta da simplicidade e familiaridade do autor com a biblioteca.

Das implementações de extração: como a estratégia inicial visa a extração em
páginas HTML, \cite{lib:scrapy} foi utilizada por resolver essa demanda com
recursos que visam facilitar tarefas comuns em raspagem de dados, enquanto a
extração via API JSON utilizou-se puramente de recursos nativos da biblioteca
padrão de Python. Do armazenamento, a \textit{cache} foi implementada como um
banco de dados SQLite~\cite{tool:sqlite}. Da exportação, foram utilizados os
formatos JSONLines como intermediário (visando facilitar a sua manipulação por
outras bibliotecas e ser simples de gerar e exportar para outros formatos) e
XLSX para visualização e manipulação por usuários via outros softwares que
suportem o formato.


\section{Implementação das estratégias}

Toda consulta por processos por meio da \tjscraper~corresponde a um processo
completo de extração dos dados dos processos dos TJs, sequenciado nos seguintes
passos:

\begin{enumerate}
    \item Normalização dos parâmetros de busca a partir da entrada do usuário
        para um formato interno padronizado da ferramenta \tjscraper. Os
        parâmetros implementados no momento da publicação deste trabalho são:

        \begin{itemize}
            \item Intervalo de número dos processos, dados em termos do valor
                inicial e final para o campo ``NNNNNNN'' da numeração
                unificada;
            \item Tribunal de Justiça a qual os processos pertencem;
            \item Ano de distribuição dos processos;
            \item Palavras a serem filtradas. Quando omitido, este parâmetro é
                ignorado.
        \end{itemize}
    \item \label{item:pré-filtragem} Pré-filtragem dos dados averiguando quais
        dos processos no intervalo de busca já estão na \textit{cache}, já
        aplicando os demais parâmetros de busca. De resultado, dados
        previamente salvos são já exportados e um sub-conjunto com os demais
        números de processos é enviado para o passo
        do~\cref{item:chamada-de-função}.
    \item \label{item:chamada-de-função} Chamada da função de extração conforme
        a estratégia de extração utilizado (HTML ou API JSON);
    \item Exportação dos dados advindos do~\cref{item:chamada-de-função} no
        formato escolhido pelo usuário suportado pela ferramenta.
\end{enumerate}

As estratégias enunciadas no~\Cref{chp:construção-da-ferramenta} se aplicam
aos~\cref{item:pré-filtragem,item:chamada-de-função}, nos quais são feitas
respectivamente as consultas à \textit{cache} e as requisições aos servidores
dos TJs.

\subsection{Extração HTML}

Para a extração dos dados em páginas HTML, os passos da extração foram
determinados como:

\begin{enumerate}
    \item Dado um intervalo de números de processos (no formato unificado ou
        antigo), as URLs correspondentes a cada um dos valores nele são
        montadas e repassadas para uma \textit{spider}\footnote{Mecanismo
        interno do Scrapy que reorganiza, escalona e efetua requisições e
        repassa à função de tratamento de suas respostas.} do Scrapy;
    \item \label{step-1} Cada URL passada irá eventualmente, em alguma ordem, chamar o método
        \texttt{parse} da \textit{spider}, e nele é extraído o conteúdo de uma
        página seguindo os passos:

        \begin{enumerate}
            \item Identificar se a página é de um processo inválido,
                inexistente ou se há a exigência de preenchimento de um
                \textit{captcha}.
            \item Caso seja um processo válido, buscam-se na página os campos
                desejados utilizando
        \end{enumerate}
\end{enumerate}

Para os passos do~\cref{step-1}, utilizou-se a especificação~\cite{spec:xpath}
suportada nativamente pelo Scrapy para a busca dos padrões de resultado e de
campos desejados no HTML da página.

\todo{Especificar XPath para páginas de erro.}

\newcommand{\urlProcValido}{\url{%
    http://www4.tjrj.jus.br/consultaProcessoWebV2/consultaProc.do?numProcesso=2021.004.015548-9
}}

Uma página de resultado da consulta pública do subdomínio ww4 do TJ-RJ para um
número de processo válido\footnote{\urlProcValido}
(\Cref{fig:exemplo-pagina-ww4}) apresenta os campos dos dados do processo em
uma \textit{tag} HTML \texttt{<table>}, de forma que cada linha visual
corresponde a uma \textit{tag} \texttt{tr} composta por duas \textit{tags}
\texttt{td}: uma para o nome do campo e outra para o valor
(\Cref{cod:html-assunto}). Assim, um campo ``A'' pode ser encontrado procurando
por uma \texttt{td} cujo conteúdo seja ``A:'', e o valor é o próximo vizinho em
relação à hierarquia do HTML da página. A expressão XPath que representa esse
padrão é utilizada na função \mintinline{python}{extract_field(field_text)}
(\Cref{cod:extract_field}), que extrai o valor de um campo arbitrário em uma
resposta contendo uma página HTML conforme a seguinte ideia:

\begin{enumerate}
    \item \texttt{//}: Define que o item buscado pode estar em qualquer ponto
        do documento. Uma busca genérica dessa forma evita que pequenas
        variações na hierarquia HTML da página prejudiquem a busca.
    \item \texttt{td[text()='TEXTO:']}: Busca e seleciona um item da
        \textit{tag} \texttt{<td>} que tenha como conteúdo exatamente a
        \textit{string} ``TEXTO:''.
    \item \texttt{/following-sibling::td}: Se foi encontrado o item, então será
        seleciona o próximo vizinho de mesmo nível hierárquico que seja da
        \textit{tag} \texttt{<td>}.
    \item \texttt{/text()}: Desse item vizinho, captura-se o conteúdo dele como
        \textit{string}. Por ser o último item do XPath, então esse será o
        valor retornado pela busca.
\end{enumerate}


Finalizada a extração dos campos e tendo então os dados de um processo, eles
são então exportados como uma linha adicional em um arquivo no
formato~\cite{spec:jsonlines}. Ao final de uma varredura por um intervalo de
número de processos, o arquivo contém os dados de todos os processos processos
dele.

\begin{listing}
    \centering{}
    \begin{minted}[autogobble,breaklines]{html}
        <tr>
          <td class="info" valign="top" nowrap="nowrap">Assunto:</td>
          <td valign="top">
            Incapacidade Laborativa Permanente / Auxílio-Acidente (Art. 86) / Benefícios em Espécie
          </td>
        </tr>
    \end{minted}
    \caption{Código HTML do campo ``Assunto:'' presente na~\Cref{fig:exemplo-pagina-ww4}.}
    \label{cod:html-assunto}
\end{listing}

\begin{listing}[htb]
    \centering{}
    \begin{minted}[breaklines]{python}
        def extract_field(response: Response, field_text: str) -> str:
            field_xpath = f"//td[text()='{field_text}:']/following-sibling::td/text()"
            return response.xpath(field_xpath).get().strip()
    \end{minted}
    \caption{%
        Código da função responsável pela extração de um campo em uma resposta
        de uma requisição a uma página de visualização de processo.
    }
    \label{cod:extract_field}
\end{listing}


\subsection{JSON}

\review{Os processos são exportados em um arquivo de resultados em formato~\cite{spec:jsonlines}}

\subsection{Assíncrona}

\review{nativo de Python 3.5+, baseado em Corrotinas com \texttt{async/await}~\cite{spec:pep0492},}

Como forma de evitar sobrecarga dos servidores dos TJs por um excesso de
requisições em um período curto de tempo, evitando possíveis sanções de algum
deles ou mesmo consumo excessivo de memória da máquina cliente devido à
quantidade de objetos das Corrotinas instanciadas para as requisições, foi
implementado um mecanismo limite de requisições simultâneas. O mecanismo
funciona separando os IDs dos processos a serem buscados em ``passos'', em que
cada passo dispara um número fixo de requisições e o próximo passo só é
executado quando todas do atual tiverem sido tratadas. O tamanho (quantidade de
processos por passo) adequado de passo escolhido como padrão foi de
\todofootnote{100 processos}{Escolher valor melhor quando forem apresentados os
resultados}. Resultados de ganhos para diferentes tamanhos de passo estão
expostos na~\todo{\Cref{gra:tempos-tamanhos-de-passo-async}}
(\Cref{chp:Resultados-Experimentais}).

\todo{Fazer comparativos para um intervalo grande de processos (100, 1000, 10000).}

\begin{figure}[htb]
    \centering
    \begin{tikzpicture}
        \pgfplotstableread{io_stats-batch-size.csv}{\table}
        \pgfplotstablegetcolsof{\table}
        \pgfmathtruncatemacro\numberofcols{\pgfplotsretval-1}
        \pgfplotstablegetcolumnnamebyindex{0}\of{\table}\to{\colprincipal}
        \begin{axis}[
            ybar,
            enlarge x limits=0.15,
            symbolic x coords={
                10, 100, 500, 1000
            },
            ylabel={Tempo (s)},
            xlabel={Tamanho do bloco},
            legend cell align=left,
            legend style={
                legend pos=outer north east,
                cells={align=left},
            },
            xtick=data,
            xticklabels from table={\table}{\colprincipal},
            width=0.7\textwidth,
            clip=false,
        ]
            \pgfplotsinvokeforeach{1,...,\numberofcols}{
                \pgfplotstablegetcolumnnamebyindex{#1}\of{\table}\to{\colname}
                \addplot table [y index=#1] {\table};
                \addlegendentryexpanded{\colname}
            }
        \end{axis}
    \end{tikzpicture}
    \caption{%
        Comparação do tempo de obtenção de todos os processos de diferentes intervalos conforme tamanho do bloco.
    }
    \label{gra:tempos-tamanhos-de-passo-async}
\end{figure}

\todo{%
    Em vez de gerar gráfico de sync/async (mover este para a parte de
    resultados), gerar um gráfico para IO/step\_size.
}

Para melhor visualização da implementação,
a~\Cref{lst:requisição-processos-síncrona-e-assíncrona} reproduz, abstraindo as
operações que não dizem respeito à programação assíncrona, o procedimento de
busca por processos respectivamente de forma síncrona e assíncrona.

\begin{listing}[htb]
    \tiny
    \begin{minipage}[t]{0.5\textwidth}
        \begin{minted}[gobble=12,breaklines]{python}
            def fetch_process(
                session: requests.Session,
                url: str,
                id_: str,
            ) -> dict[str, Any]:
                """
                Busca um único processo com o número dado em
                `id_` utilizando a API em `url`, filtrando
                processos por um critério arbitrário (ex:
                se contém um assunto específico).
                """
                response = session.post(
                    url, json={"numProcesso": id_}
                )

                return classify_process(response.json())

            def download_processes(
                url: str,
                ids: list[str],
                sink: Path,  # Onde os processos serão salvos
            ) -> None:
                """Salva todos os processos com determinados IDs em um arquivo JSONL."""
                with requests.Session() as session:
                    for id_ in ids:
                        process = fetch_process(session, id)
                        save_process(process, sink)
        \end{minted}
    \end{minipage}
    \begin{minipage}[t]{0.6\textwidth}
        \begin{minted}[gobble=12,breaklines]{python}
            async def fetch_process(
                session: aiohttp.ClientSession,
                url: str,
                id_: str,
            ) -> dict[str, Any]:
                """
                Busca um único processo com o número dado em
                `id_` utilizando a API em `url`.
                """
                response = requests.post(url, json={"numProcesso": id_})

                return classify_process(response.json(), filter_function)

            async def download_processes(
                url: str,
                ids: list[str],
                sink: Path,  # Onde os processos serão salvos
                step_size: int,  # Número de processos por passo
            ) -> None:
                async with aiohttp.ClientSession() as session:
                    for step_ids in iter_in_steps(ids, step_size):
                        requests = (
                            fetch_process(session, url, step_id)
                            for step_id in step_ids
                        )
                        processes = await asyncio.gather(*requests)

                        for process in processes:
                            save_process(process, sink)
        \end{minted}
    \end{minipage}
    \caption{%
        Reprodução do procedimento de busca por processos de maneira sequencial
        (síncrona, à esquerda) e concorrente/não-bloqueante (assíncrona, à direita).
    }
    \label{lst:requisição-processos-síncrona-e-assíncrona}
\end{listing}

\subsection{Cache}

A \textit{cache} foi implementada como um instância de banco de dados
SQLite~\cite{tool:sqlite} com uma única tabela ``Processos'' (reproduzida na
\Cref{tbl:estrutura-tabela-processos}), com a descrição dos valores possíveis
para a coluna \textit{``cache\_state''} descritos
na~\Cref{tbl:valores-coluna-state}. A chave primária da tabela foi escolhida
como o número do processo seguindo a numeração unificada, visto que é uma
informação única a cada processo e, convenientemente, é o parâmetro de busca
utilizado na extração, facilitando a separação de quais processos serão
buscados na \textit{cache} e quais serão requisitados ao servidores dos TJs.

\begin{table}[htb]
    \small
    \centering
    \begin{tabular}{llp{0.6\textwidth}}
        \toprule
        \multicolumn{3}{c}{Processos} \\
        \midrule
        Coluna & Tipo & Descrição \\
        \midrule \\
        *\texttt{id} & text & Número do processo no padrão unificado. \\
        \texttt{cache\_state} & text & Estado atual do processo na \textit{cache}. \\
        \texttt{assunto} & text & Assunto do processo. \\
        \texttt{json} & text & Dados do processo no formato JSON espelhando os campos retornados pela API do ww3. \\
        \bottomrule
    \end{tabular}
    \caption{%
        Definição da tabela SQLite ``Processos'' utilizada para \textit{cache}.
        ``*'' indica que o campo é uma chave primária.
    }
    \label{tbl:estrutura-tabela-processos}
\end{table}

\begin{table}[htb]
    \centering
    \begin{tabular}{lp{0.8\textwidth}}
        \toprule
        Estado & Descrição \\
        \midrule \\
        \texttt{CACHED} & O número do processo é válido e os dados do processo estão na \textit{cache}. \\
        \texttt{INVALID} & O número do processo é inválido (por exemplo, dígitos de validação não conferem pelo algoritmo do TJ). \\
        \texttt{NOT\_CACHED} & O número do processo é válido porém os dados dele não estão na \textit{cache}. Para uso no software, não é registrado na \textit{cache}. \\
        \bottomrule
    \end{tabular}
    \caption{%
        Definição da tabela SQLite ``Processos'' utilizada para \textit{cache}.
        ``*'' indica que o campo é uma chave primária.
    }
    \label{tbl:valores-coluna-state}
\end{table}

Essa separação é uma operação trivial: para saber se um processo deve ou não
ser requisitado aos servidores dos TJs, basta para cada número de processo da
lista de números a serem buscados consultar, na \textit{cache}, se não existe
uma entrada com esse número como valor da chave primária e que o valor de
``cache\_state'' seja ``CACHED''.

Um processo sempre é salvo na \textit{cache} quando o servidor do TJ responde à
requisição referente a uma consulta por número de processo, com exceção de
casos em que a resposta é um erro de captcha.

\todo{Falar sobre quando um processo é cacheado e quando não é}

\section{Implementação das interfaces}

\subsection{}
