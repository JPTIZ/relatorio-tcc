\chapter{Implementação da Ferramenta \tjscraper~\label{chp:implementação-da-ferramenta}}

\section{Organização geral}

A ferramenta foi projetada como uma biblioteca de \textit{software} acompanhada
de duas interfaces de usuário~\footnote{Nesta seção, ``usuário'' refere-se a
alguém que não esteja utilizando a ferramenta como uma biblioteca de software,
e sim como aplicação.}: uma aplicação de Interface de Linha de Comando
(ILC\footnote{Comumente referenciado com a sigla em inglês ``CLI'', de
``Command Line Interface''.}) e uma aplicação de servidor Web.

A ILC tem como objetivo permitir o uso de boa parte dos diferentes recursos da
biblioteca em um terminal para a execução tarefas pontuais, como exibir
informações sobre estado atual da \textit{cache}, iniciar um processo de
extração, ou mesmo iniciar uma instância de servidor Web da ferramenta para
fins de desenvolvimento.

O servidor Web é destinado ao usuário que queira uma interface de simples
acesso e uso da ferramenta, primariamente focando na obtenção de dados dos TJs
com os recursos que não estejam disponíveis nas páginas oficiais dos TJs
(descritos na~\Cref{sec:Motivação-e-Contexto}), tendo em mente como público
alvo jornalistas.

\section{Ferramentas utilizadas~\label{section:ferramentas-utilizadas}}

Visando fomentar a iniciativa e movimento de Software-Livre, tomou-se como
exigência própria para a implementação que todas as ferramentas, assim como a
próprio \tjscraper, fossem Software-Livre por, além de outros fatores, questões
de reprodutibilidade e auditoria.

Para a implementação da \tjscraper, escolheu-se Python 3.10 como linguagem
principal considerando os seguintes critérios técnicos:

\begin{enumerate}
    \item A premissa de que o gargalo do tempo de busca seria o de IO de Rede
        (para busca dos processos nos servidores dos TJs) ou de Armazenamento
        (para buscas na \textit{cache});
    \item Não se tratando de um problema inerentemente atrelado à
        CPU\footnote{Tradução livre para o termo em inglês
        ``\textit{CPU-bound}''.}, não seriam prioritárias otimizações
        agressivas de compilador tanto AOT nem JIT\footnote{``Ahead-Of-Time''
        --- antes da execução do programa --- e ``Just-In-Time'' --- durante a
        execução do programa ---, respectivamente.};
    \item Familiaridade do autor com a linguagem;
    \item Disponibilidade de recursos de abstração de alto-nível na biblioteca
        padrão da linguagem ou em bibliotecas externas para as principais
        operações da ferramenta.
\end{enumerate}

O gerenciamento de dependências e isolamento do ambiente de desenvolvimento foi
feito utilizando \cite{about:poetry} visando tanto a reprodutibilidade dos
resultados e funcionamento da ferramenta quanto facilitar seu empacotamento e
distribuição. A ILC foi implementada via \cite{about:typer} pelas garantias de
corretude de tipagem. Para a interface Web, foi escolhido \cite{about:flask}
por conta da simplicidade e familiaridade do autor com a biblioteca.

Das implementações de extração: como a estratégia inicial visa a extração em
páginas HTML, \cite{lib:scrapy} foi utilizada por resolver essa demanda com
recursos que visam facilitar tarefas comuns em raspagem de dados, enquanto a
extração via API JSON utilizou-se puramente de recursos nativos da biblioteca
padrão de Python em boa parte das operações e a biblioteca~\cite{lib:aiohttp}
para as requisições assíncronas. Do armazenamento, a \textit{cache} foi
implementada como um banco de dados~\cite{tool:sqlite}. Da exportação, foram
utilizados os formatos JSONLines~\cite{spec:jsonlines} como intermediário
(visando facilitar a sua manipulação por outras bibliotecas e ser simples de
gerar e exportar para outros formatos) e XLSX para visualização e manipulação
por usuários via outros softwares que suportem o formato.


\section{Implementação das estratégias}

Toda consulta por processos por meio da \tjscraper~corresponde a um processo
completo de extração dos dados dos processos dos TJs, sequenciado nos seguintes
passos:

\begin{enumerate}
    \item Normalização dos parâmetros de busca a partir da entrada do usuário
        para um formato interno padronizado da ferramenta \tjscraper. Os
        parâmetros implementados no momento da publicação deste trabalho são:

        \begin{itemize}
            \item Intervalo de número dos processos, dados em termos do valor
                inicial e final para o campo ``NNNNNNN'' da numeração
                unificada;
            \item Tribunal de Justiça a qual os processos pertencem;
            \item Ano de distribuição dos processos;
            \item Palavras a serem filtradas. Quando omitido, este parâmetro é
                ignorado.
        \end{itemize}
    \item \label{item:pré-filtragem} Pré-filtragem dos dados averiguando quais
        dos processos no intervalo de busca já estão na \textit{cache}, já
        aplicando os demais parâmetros de busca. De resultado, dados
        previamente salvos são já exportados e um sub-conjunto com os demais
        números de processos é enviado para o passo
        do~\cref{item:chamada-de-função}.
    \item \label{item:chamada-de-função} Chamada da função de extração conforme
        a estratégia de extração utilizado (HTML ou API JSON);
    \item Exportação dos dados advindos do~\cref{item:chamada-de-função} no
        formato escolhido pelo usuário suportado pela ferramenta.
\end{enumerate}

As estratégias enunciadas no~\Cref{chp:construção-da-ferramenta} se aplicam
aos~\cref{item:pré-filtragem,item:chamada-de-função}, nos quais são feitas
respectivamente as consultas à \textit{cache} e as requisições aos servidores
dos TJs.

\subsection{Extração HTML}

Para a extração dos dados em páginas HTML, os passos da extração foram
determinados como:

\begin{enumerate}
    \item Dado um intervalo de números de processos (no formato unificado ou
        antigo), as URLs correspondentes a cada um dos valores nele são
        montadas e repassadas para uma \textit{spider}\footnote{Mecanismo
        interno do Scrapy que reorganiza, escalona e lança as requisições
        repassando suas respostas à função de tratamento.} do Scrapy;
    \item \label{step-1} Cada URL passada irá eventualmente, em alguma ordem, chamar o método
        \texttt{parse} da \textit{spider}, e nele é extraído o conteúdo de uma
        página seguindo os passos:

        \begin{enumerate}
            \item Identificar se a página é de um processo inválido,
                inexistente ou se há a exigência de preenchimento de um
                \textit{captcha}.
            \item Caso seja um processo válido, buscam-se na página os campos
                desejados utilizando
        \end{enumerate}
\end{enumerate}

Para os passos do~\cref{step-1}, utilizou-se a especificação~\cite{spec:xpath}
suportada nativamente pelo Scrapy para a busca dos padrões de resultado e de
campos desejados no HTML da página avaliando a estrutura interna da página.

Uma página de resultado da consulta pública do subdomínio ww4 do TJ-RJ para um
número de processo válido apresenta os campos dos dados do processo em uma
\textit{tag} HTML \texttt{<table>} com cada linha visual correspondendo a uma
\textit{tag} \texttt{tr} composta por duas \textit{tags} \texttt{td}: uma para
o nome do campo e outra para o valor (\Cref{cod:html-assunto}). Assim, um campo
``A'' pode ser encontrado procurando por uma \texttt{td} cujo conteúdo seja
``A:'', e o valor é o próximo vizinho em relação à hierarquia do HTML da
página. A expressão XPath que representa esse padrão é utilizada na função
\mintinline{python}{extract_field(field_text)} (\Cref{cod:extract_field}), que
extrai o valor de um campo arbitrário em uma resposta contendo uma página HTML
conforme a seguinte ideia:

\begin{enumerate}
    \item \texttt{//}: Define que o item buscado pode estar em qualquer ponto
        do documento. Uma busca genérica dessa forma evita que pequenas
        variações na hierarquia HTML da página prejudiquem a busca.
    \item \texttt{td[text()='TEXTO:']}: Busca e seleciona um item da
        \textit{tag} \texttt{<td>} que tenha como conteúdo exatamente a
        \textit{string} ``TEXTO:''. Na função \texttt{extract\_field},
        ``TEXTO'' é substituído pelo argumento enviado ao parâmetro
        \texttt{field\_text}.
    \item \texttt{/following-sibling::td}: Se foi encontrado o item, então será
        seleciona o próximo vizinho de mesmo nível hierárquico que seja da
        \textit{tag} \texttt{<td>}.
    \item \texttt{/text()}: Desse item vizinho, captura-se o conteúdo dele como
        \textit{string}. Por ser o último item do XPath, então esse será o
        valor retornado pela busca.
\end{enumerate}


\begin{listing}
    \centering{}
    \begin{minted}[autogobble,breaklines]{html}
        <tr>
          <td class="info" valign="top" nowrap="nowrap">Assunto:</td>
          <td valign="top">
            Incapacidade Laborativa Permanente / Auxílio-Acidente (Art. 86) / Benefícios em Espécie
          </td>
        </tr>
    \end{minted}
    \caption{Código HTML do campo ``Assunto:'' presente na~\Cref{fig:exemplo-pagina-ww4}.}
    \label{cod:html-assunto}
\end{listing}


Para a detecção de páginas de erro, as expressões correspondentes e a ação que
é realizada em cima delas para verificação do erro estão descritas
na~\Cref{tbl:xpaths-de-erros}.

\begin{table}[htb]
    \begin{tabular}{lll}
        \toprule
        Erro & Verificação complementar & XPath \\
        \midrule
        Página de \textit{Captcha} & Existência do elemento. & \mintinline{text}{//*[@id="container_captcha"]} \\
        Número inválido & Valor é exatamente ``erro''. & \texttt{//title/text()} \\
        \bottomrule
    \end{tabular}
    \caption{%
        Expressões XPath e qual operação de verificação é realizada em cima de
        seus resultados para verificação de erro.
    }
    \label{tbl:xpaths-de-erros}
\end{table}


% Finalizada a extração dos campos e tendo então os dados de um processo, eles
% são então exportados como uma linha adicional em um arquivo no
% formato~\cite{spec:jsonlines}. Ao final de uma varredura por um intervalo de
% número de processos, o arquivo contém os dados de todos os processos processos
% dele.

\begin{listing}[htb]
    \centering{}
    \begin{minted}[breaklines]{python}
        def extract_field(response: Response, field_text: str) -> str:
            field_xpath = f"//td[text()='{field_text}:']/following-sibling::td/text()"
            return response.xpath(field_xpath).get().strip()
    \end{minted}
    \caption{%
        Código da função responsável pela extração de um campo em uma resposta
        de uma requisição a uma página de visualização de processo.
    }
    \label{cod:extract_field}
\end{listing}

\subsection{Extração de processos via API JSON e requisições assíncronas}

A função \mintinline{python}{download_with_json_api} do módulo
\texttt{tj\_scraper.download} abstrai o processo de extração de dados de
processos pela API JSON de TJs que conhecidamente a tenham. A implementação
dessa função já faz proveito da estratégia de requisições assíncronas descrita
na~\Cref{sub:requisições-assíncronas} e tem como parâmetros: uma especificação
das combinações de números de processo na numeração unificada que serão
testadas, composta pelo intervalo de valores que serão utilizados para
preencher o campo ``NNNNNNN'', o TJ em questão e o ano; o arquivo
JSONLines~\cite{spec:jsonlines} de saída que será preenchido com os dados dos
processos extraídos; o caminho para a \textit{cache}; a função que será
utilizada para filtrar os resultados; e o tamanho dos ``lotes'' (explicados
mais à frente). O TJ passado por parâmetro contém as definições de suas rotas e
unidades de origem previamente carregadas a partir de um arquivo
TOML~\cite{spec:toml}. A definição das rotas é dada por uma rota para a
numeração unificada e outra para a numeração antiga.

Adentrando na implementação do processo de extração, cuja ideia geral
simplificada está resumida no~\Cref{alg:iteração-por-combinações}, criam-se via
expressão geradora\footnote{Mecanismo de Python para iteração através de
avaliação preguiçosa.} todas as requisições que serão enviadas ao servidor do
TJ a partir da iteração pela especificação de combinações. Cada iteração gera
uma combinação parcial em que se fixa, além dos demais campos previamente
fixados na chamada da função\footnote{Mais claramente, como já são passados um
TJ e um ano específico, os campos fixados previamente são ``J'', ``TR'' e
``AAAA''.}, um valor para o campo ``NNNNNNN'' dado sequencialmente até que se
chegue ao último valor do intervalo passado por parâmetro, mantendo variáveis
apenas os campos de unidade de origem (``OOOO'') e dígitos de verificação
(``DD'').

Em seguida, para cada combinação parcial, varia-se o campo de ``OOOO'' para
cada uma das unidades de origem do TJ em questão, ordenadas do menor para o
maior número. A cada variação, monta-se a \textit{string} representante da
numeração unificada e, então, uma primeira requisição de teste é enviada à rota
correspondente do TJ. Utilizando a~\Cref{tbl:respostas-ww3} como referência, a
resposta do servidor é classificada em um resultado de erro específico ou um
processo válido. No caso de um processo válido, uma requisição adicional é
enviada para a rota da numeração antiga do TJ para se extrair os campos úteis
do processo conforme a~\Cref{tbl:relação-chaves-campos-json}. Para processos
com múltiplas instâncias, apenas o da primeira instância é utilizado.

\begin{algorithm}
    \Entrada{$N_i \geq 0, N_f \geq 0, A \geq 0$, um $TJ$ e uma função $Filtro$}
    \Saida{Conjunto de processos $P$}
    \ParaCada{$N \in [N_i, N_f]$}{
        \ParaCada{$O \in UnidadesDeOrigem(TJ)$}{
            $D \gets Modulo97(N, A, 8, Codigo(TJ))$\;
            Gere $Num_{CNJ}$ como a numeração unificada preenchida com $(N, D, A, 8, Codigo(TJ), O)$\;
            Envie uma requisição $R_1$ para a rota de numeração unificada de $TJ$ com $Num_{CNJ}$\;
            \Se{$R_1$ responder com um processo válido}{
                Extraia o campo de numeração antiga de $R_1$ e salve-o em $Num_{Antiga}$\;
                Envie uma requisição $R_2$ para a rota de numeração antiga de $TJ$ com $Num_{Antiga}$\;
                \Se{$R_2$ responder com sucesso}{
                    Salve o processo contido em $R_2$ na \textit{cache} como válido\;
                    \Se{$Filtro(R_2) = falso$}{
                        Exporte $R_2$\;
                    }
                    Pare o \textit{loop} interno\;
                }
            }
            Salve o processo contido em $R_2$ na \textit{cache} como inválido\;
        }
    }
    \caption{%
        Extração de processos via API JSON (simplificada).
    }
    \label{alg:iteração-por-combinações}
\end{algorithm}

% Async

Para realizar o disparo das requisições de forma assíncrona implementando a
estratégia descrita na~\Cref{sub:requisições-assíncronas}, as funções internas
chamadas pela \mintinline{python}{download_with_json_api} foram definidas
segundo o modelo \textit{async/await} da biblioteca padrão de Python. O modelo,
nativo de Python 3.5+ baseado em Corrotinas~\cite{spec:pep0492}, é responsável
pelo caráter de IO não-bloqueante nas requisições e exige, para tal e portanto,
que a biblioteca de requisições seja compatível com ele, justificando o uso da
AIOHTTP. A~\Cref{cod:requisição-processos-síncrona-e-assíncrona} reproduz, com
as devidas simplificações, o código da função responsável pela tentativa de
descoberta de um processo a partir de um único número em que o método
\mintinline{python}{aiohttp.ClientSession.post} envia uma requisição POST de
forma assíncrona. Foi utilizado um objeto de sessão para reaproveitamento da
conexão, visto que a intenção é disparar múltiplas requisições.

\begin{listing}[htb]
    \centering
    \tiny
    \begin{minted}[breaklines]{python}
        async def fetch_process(
            session: aiohttp.ClientSession,
            cnj_number: CNJProcessNumber,
            tj: TJ,
        ) -> FetchResult:
            cnj_number_str = make_cnj_number_str(cnj_number)

            request_args = TJRequestParams(tipoProcesso="1", codigoProcesso=cnj_number_str)

            # Descoberta do processo
            raw_response: TJResponse
            async with session.post(
                tj.cnj_endpoint,
                json=request_args,
                ssl=False,
            ) as response:
                raw_response = json.loads(await response.text())

            fetch_result = classify(raw_response, cnj_number, tj)

            if isinstance(fetch_result, FetchFailReason):
                return fetch_result

            request_args = TJRequestParams(
                tipoProcesso=str(fetch_result.get("tipoProcesso")),
                codigoProcesso=str(fetch_result.get("numProcesso")),
            )

            # Extração de todos os campos
            async with session.post(
                tj.main_endpoint,
                json=request_args,
                ssl=False,
            ) as response:
                raw_response = json.loads(await response.text())

            return classify(raw_response, cnj_number, tj)
    \end{minted}
    \caption{%
        Reprodução do procedimento de busca por processos de maneira
        assíncrona.
    }
    \label{cod:requisição-processos-síncrona-e-assíncrona}
\end{listing}

% Batches/semaphore

Como forma de evitar sobrecarga dos servidores dos TJs por um excesso de
requisições em um período curto de tempo causado pela implementação assíncrona,
o que arriscaria sanções de algum dos servidores ou mesmo consumo excessivo de
memória da máquina cliente devido à quantidade de objetos das Corrotinas
instanciadas para as requisições, foi implementado um mecanismo limite de
requisições simultâneas. O mecanismo funciona iterando pelas combinações em
``lotes'' de tamanho fixo, em que cada lote dispara um número máximo de
requisições e o próximo lote só é executado quando todas do atual tiverem sido
tratadas.

É necessário notar pela implementação que a iteração em blocos é sobre as
combinações parciais --- ou seja, variando pelo valor do campo ``NNNNNNN'', de
forma a ser interpretada como ``tentativa de se descobrir se existem um números
de processo válidos dados determinados valores para o campo ``NNNNNNN''''.
Sendo assim, um semáforo iniciado com o tamanho do lote é utilizado para
garantir que as variações internas para o campo ``OOOO'' das combinações
parciais não irão acidentalmente ultrapassar o limite do tamanho do lote.

Em busca de alguma informação sobre limites de tráfego definidos pelo TJ-RJ,
tentou-se procurar se alguma dos caminhos intermediários da rota de consulta
processual fornecia acesso a um arquivo \texttt{robots.txt} (comumente
utilizado para especificar limitações para softwares como indexadores de
busca), porém em nenhuma se obteve sucesso. Logo, para a estimava de tamanho de
lote se limitou arbitrariamente a no máximo 1000 processos por lote.

\subsection{Estrutura e mecanismo de \textit{Caching}}

A \textit{cache} foi implementada como um instância de banco de dados
SQLite~\cite{tool:sqlite} com uma única tabela ``Processos'' (reproduzida na
\Cref{tbl:estrutura-tabela-processos}), com a descrição dos valores possíveis
para a coluna \textit{``cache\_state''} descritos
na~\Cref{tbl:valores-coluna-state}. A chave primária da tabela foi escolhida
como o número do processo seguindo a numeração unificada, visto que é uma
informação única a cada processo e, convenientemente, é o parâmetro de busca
utilizado na extração, facilitando a separação de quais processos serão
buscados na \textit{cache} e quais serão requisitados ao servidores dos TJs.

Essa separação é uma operação trivial: para saber se um processo deve ou não
ser requisitado aos servidores dos TJs, basta para cada número de processo da
lista de números a serem buscados consultar, na \textit{cache}, se não existe
uma entrada cuja chave primária se diferencie apenas nos campos ``DD'' e
``OOOO'' e que o valor de ``cache\_state'' seja ``CACHED''.

\begin{table}[htb]
    \small
    \centering
    \begin{tabular}{llp{0.6\textwidth}}
        \toprule
        \multicolumn{3}{c}{Processos} \\
        \midrule
        Coluna & Tipo & Descrição \\
        \midrule \\
        *\texttt{id} & text & Número do processo no padrão unificado. \\
        \texttt{cache\_state} & text & Estado atual do processo na \textit{cache}. \\
        \texttt{assunto} & text & Assunto do processo. \\
        \texttt{json} & text & Dados do processo no formato JSON espelhando os campos retornados pela API do ww3. \\
        \bottomrule
    \end{tabular}
    \caption{%
        Definição da tabela SQLite ``Processos'' utilizada para \textit{cache}.
        ``*'' indica que o campo é uma chave primária.
    }
    \label{tbl:estrutura-tabela-processos}
\end{table}

\begin{table}[htb]
    \centering
    \begin{tabular}{lp{0.8\textwidth}}
        \toprule
        Estado & Descrição \\
        \midrule \\
        \texttt{CACHED} & O número do processo é válido e os dados do processo estão na \textit{cache}. \\
        \texttt{INVALID} & O número do processo é inválido (por exemplo, dígitos de validação não conferem pelo algoritmo do TJ). \\
        \texttt{NOT\_CACHED} & O número do processo é válido porém os dados dele não estão na \textit{cache}. Para uso no software, não é registrado na \textit{cache}. \\
        \bottomrule
    \end{tabular}
    \caption{%
        Definição da tabela SQLite ``Processos'' utilizada para \textit{cache}.
        ``*'' indica que o campo é uma chave primária.
    }
    \label{tbl:valores-coluna-state}
\end{table}


% TODO: talvez em algum ponto seja interessante colocar que:
%
%  Especificação da combinação
%             |
%             v
%     Combinação parcial
%             |
%             v
%         Combinação

Um processo sempre é salvo na \textit{cache} quando o servidor do TJ responde à
requisição referente a uma consulta por número de processo, com exceção de
casos em que a resposta é um erro de captcha. Ou seja, mesmo que durante uma
consulta um processo seja filtrado, para referência e aceleração de consultas
futuras ele ainda assim é salvo na \textit{cache}. Da mesma forma, combinações
que não gerem números de processos existentes também são salvas na
\textit{cache} com o estado \texttt{INVALID}.

Consultas à \textit{cache} foram implementadas com consultas SQL aproveitando o
registro de funções definidas em Python para as operações de filtragem,
conforme código reproduzido na~\Cref{cod:restauração-dados-processos-cache}.

\begin{listing}[htb]
    \tiny
    \centering
    \begin{minted}[autogobble,breaklines]{python}
    def restore_json_for_ids(
        cache_path: Path,
        ids: list[CNJProcessNumber],
        filter_function: Callable[[DBProcess], bool],
    ) -> list[ProcessJSON]:
        if not cache_path.exists():
            raise FileNotFoundError(cache_path)

        def is_id_in_list(id_: str) -> bool:
            return to_cnj_number(id_) in ids

        def custom_filter(number: str, state: CacheState, subject: str, json_str: str) -> bool:
            try:
                process = DBProcess(
                    number, cache_state=state, subject=subject, json=json.loads(json_str)
                )
                return filter_function(process)
            except Exception as error:
                print(f"Failed to use custom filter: {error}")
                raise

        with sqlite3.connect(cache_path) as connection:
            connection.create_function("is_in_list", 1, is_id_in_list)
            connection.create_function("custom_filter", 4, custom_filter)
            cursor = connection.cursor()

            return [
                json.loads(item_json)
                for item_json, in cursor.execute(
                    "select json from Processos"
                    " where is_in_list(id)"
                    " and custom_filter(id, cache_state, subject, json)",
                )
            ]

        return []
    \end{minted}
    \caption{%
        Reprodução do código de restauração dos dados de processos da
        \textit{cache} a partir de filtros personalizados.
    }
    \label{cod:restauração-dados-processos-cache}
\end{listing}

% \section{Implementação das interfaces}
%
% % TODO: Simplificar: mostrar como ficaram, e mostrar código só se necessário.
%
% \subsection{}
